When using off-policy with a behaviour policy that picks actions uniformly at random the chance of any learning signal reaching one of the starting states is very small. Remember that we learn from tails of episodes and have to cut it short when the behaviour policy picked a different action from what the target policy would have picked (W becomes 0). Since there are 9 actions and it takes multiple actions even for an optimal policy to reach the goal, this is not feasable for a policy that picks actions uniformly at random.

To run the off-policy version, the behaviour policy is therefore updated after each episode as well. Currently, it is an epsilon greedy policy and the off-policy MC control improves it using on-policy MC. It has its own Q function for this, so it does NOT use the Q function of the target policy, which is also being estimated in parallell. I'm unsure of the convergence properties if the behaviour policy were to use the target policy's Q function. It seems like a good idea intuitively - we want it to pick the same actions that the target policy would have picked (to give a learning signal), but sometimes explore instead. However, the moving targets and non-stationarity in multiple distributions might make it difficult in practice. Regardless, an advantage of using a separate Q function is that the behaviour policy can improve even if it picked an action different from the target policy. Therefore it improves faster in the early states which also means it will reach the goal faster which in turn creates more learning signals for the earlier states for the targets Q function.

The "Path" images show the path of the final policy. The ground has a value of 0 if it is out of bounds, otherwise 10. The path taken by the agent is given values 10+t (where t is the time step when the agent was in that cell). The 6 images are from the 6 different starting positions.
