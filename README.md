# rl-an-introduction
Small tests of the theory presented in: http://incompleteideas.net/book/RLbook2020.pdf

For reference, the origin of the "hack" used by some code files to import uninstalled packages in parent directories was taken from here: https://stackoverflow.com/questions/714063/importing-modules-from-parent-folder#comment23054549_11158224

## Future improvements
* Chapter 2
  - [x] The environments in chapter 2 should take more parameters in their constructor such as the starting mean.
  - [x] The utilities framework in chapter 2 was not designed with *Gradient Bandits* in mind. The implementation for it is therefore currently a stand-alone class in the *agent.py* file.
  - [x] The results of *2.10 Parameter Study Moving* used a very low NUM_RUNS because of the significant time for a single run. Therefore, the results are not reliable until it is rerun with a much higher NUM_RUNS.
* Chapter 5
  - [ ] Rerun the experiments to make sure code changes didn't break them.
  - [ ] I realised something very interesting with off-policy MC Control in *5.7 Racetrack MC Control*. Having initial Q-values as 0 is highly optimistic, which means an unexplored state-action pair will always be the greedy action when available (the environment gives you -1 as a reward on each timestep). This can be problematic for this kind of off-policy learning since the learning signal for state s becomes 0 if the behaviour policy picked a different action than what the target policy says after visiting s. This means that even after training for a long time, there might still be a few state-action pairs that have not gotten a learning signal through (even if the behaviour policy is improved as well), which makes it very likely that the deterministic target policy won't ever reach the finish line. Currently the fix is to ensure that we have pessimistic values so that the greedy action is always of a state-action pair that has been explored (which is equivalent to ignoring unexplored ones when performing greedy action selection). The fact that this solves a problem proves that we haven't trained enough to get a good estimate for every state-action pair. Therefore we won't get optimal solutions but they will be good given our limited Q function. A future experiment could instead give a reward of +1 when reaching the finish line and 0 otherwise and have a discount factor < 1 to incentivise speed while allowing 0 to be pessimistic. Then you could also use *Discounting-aware Importance Sampling*.

## Ideas
* Chapter 3
  * (When reading 3.2) Giving a reward of -1 at every time step to encourage the agent to escape a maze seems suboptimal from a sense of how we want the agent to "think", although it works great in the tabular case. Even though it does receive -1, it has no idea that it can do better or that there is a goal in the end with a different reward (or just a terminal state at all). When dealing with sparse rewards (I'm mostly thinking about just giving a different reward when transitioning to a terminal state), perhaps it makes sense to let the agent know the reward function and see the states (or something similar) where the reward differs. From there it could perhaps construct some model or internal reward system to encourage exploring and making "progress". The point of this whole idea is that if we only have one sparse reward at the end (and all the others are the same such as -1 or 0 for example), it will almost always make sense to let the agent know when that occurs or at least that it can occur (even if I'm not sure how it can be exploited yet). That's what we would do if a human were to try to complete such a task in most relevant situations.
